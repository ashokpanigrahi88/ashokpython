{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Spark DataFrames API\n",
    "\n",
    "In this notebook exercise, we are going to mainly focus on understanding how DataFrames API work and how spark internals work in general. \n",
    "\n",
    "We start with reading data using the DataSources API and walking through different parts of the DataFrame API to demonstrate how it can be used to process big data. \n",
    "\n",
    "Later we will move to more advanced operations in the DataFrames API and then dive into some internals and the problem of data skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session \n",
    "\n",
    "In Apache Spark 2.0 and later versions, Spark introduced a Context variable that belongs to `SparkSession` class type.\n",
    "\n",
    "In earlier versions of Apach Spark, There were two main context variables available.\n",
    "1. `SparkContext` that enabled RDD API operations\n",
    "2. `SQLContext` that enables DataFrame API and SparkSQL operations\n",
    "\n",
    "In Spark 2.0 and above versions, Apache Spark officially introduced a `SparkSession` Class that allows the user to keep both Spark Context and SQL Context in the same context variable simplifying the developer experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://surfacepro:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ae8a373848>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://surfacepro:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext # RDD API entrypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data using Data Sources\n",
    "\n",
    "DataFrames API uses datasources to read files from distributed/non-distributed data sources. Let's read some JSON string records to start with. Spark also allows using POSIX wildcards when expressing a collection of filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/DataScienceTraining/ADS06/03-spark_dataframes/data/reviews_fashion.json', 'D:/DataScienceTraining/ADS06/03-spark_dataframes/data/reviews_electronics.json', 'D:/DataScienceTraining/ADS06/03-spark_dataframes/data/reviews_sports.json']\n"
     ]
    }
   ],
   "source": [
    "filepath='D:/DataScienceTraining/ADS06/03-spark_dataframes/data/'\n",
    "reviews_paths =[filepath+'reviews_fashion.json', \n",
    "                filepath+'reviews_electronics.json',\n",
    "                filepath+'reviews_sports.json']\n",
    "\n",
    "print(reviews_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='0000031887', helpful=[0, 0], overall=5.0, reviewText='Perfect red tutu for the price. I baught it as part of my daughters Halloween costume and it looked great on her.', reviewTime='11 4, 2013', reviewerID='A2XVJBSRI3SWDI', reviewerName='abigail', summary='Nice tutu', unixReviewTime=1383523200),\n",
       " Row(asin='0000031887', helpful=[1, 1], overall=4.0, reviewText='This was a really cute tutu the only problem is that it was super short on my 5 yr old daughter. Other than that it was really adorable.', reviewTime='05 26, 2012', reviewerID='A2G0LNLN79Q6HR', reviewerName='aj_18 \"Aj_18\"', summary='Really Cute but rather short.', unixReviewTime=1337990400)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_reviews = spark.read.json(reviews_paths)\n",
    "inferred_reviews.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Define a POSIX representation for the above 3 files and read the reviews data to verify Spark reads 30,000 records. (5 mins)\n",
    "\n",
    "Hint: [POSIX Cheatsheet](https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable and call it \"POSIX\". The assign it with a string value representing \n",
    "# the filepath pattern that will capture all three .json files\n",
    "\n",
    "POSIX = filepath+\"reviews_*.json\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_reviews = spark.read.json(POSIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin='0132793040', helpful=[1, 1], overall=5.0, reviewText='Corey Barker does a great job of explaining Blend Modes in this DVD. All of the Kelby training videos are great but pricey to buy individually. If you really want bang for your buck just subscribe to Kelby Training online.', reviewTime='04 13, 2013', reviewerID='AKM1MP6P0OYPR', reviewerName='Vicki Gibson \"momo4\"', summary='Very thorough', unixReviewTime=1365811200),\n",
       " Row(asin='0321732944', helpful=[0, 0], overall=5.0, reviewText=\"While many beginner DVDs try to teach you everything there is to know about Photoshop CS5, this introductory course shows you the critical things you need to know to start feeling confident in your Photoshop skills. Bestselling author and Photoshop trainer, Matt Kloskowski shows you exactly what you need to know about tools, filters, adjustments, palettes, and menu items to hit the ground running in Photoshop. These include: layers and how they're key to mastering Photoshop, selections and the essential tools you really need to worry about, getting started with Camera Raw and which Retouching tools you'll find the most helpful. When you're done with this DVD you'll have a solid grasp of the most important features and be at the next level in no time.\", reviewTime='07 1, 2012', reviewerID='A2CX7LUOHB2NDG', reviewerName='Bernie', summary='Adobe Photoshop CS5 Crash Course with master Photoshop trainer, Matt Kloskowski', unixReviewTime=1341100800)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_reviews.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding data to a Schema\n",
    "\n",
    "From the above example, it is seen that we did not have to define a schema for the reviews dataset. Apache Spark automatically scans through the files and *infers* the schema of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicitly defining schema\n",
    "\n",
    "In certain cases, it is necessary for the schema to be defined explicitly. In instances such as:\n",
    "- missing data\n",
    "- potential future scenarios\n",
    "- explicit business constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(reviewerID,StringType,true),StructField(asin,StringType,true),StructField(reviewerName,StringType,true),StructField(helpful,ArrayType(IntegerType,true),true),StructField(reviewTime,StringType,true),StructField(overall,DoubleType,true),StructField(summary,StringType,true),StructField(unixReviewTime,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Export the modules\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define Schema\n",
    "REVIEWS_SCHEMA_DEF = StructType([\n",
    "        StructField('reviewerID', StringType(), True),\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('reviewerName', StringType(), True),\n",
    "        StructField('helpful', ArrayType(\n",
    "                IntegerType(), True), \n",
    "            True),\n",
    "#        review text column definition missing here\n",
    "        StructField('reviewTime', StringType(), True),\n",
    "        StructField('overall', DoubleType(), True),\n",
    "        StructField('summary', StringType(), True),\n",
    "        StructField('unixReviewTime', LongType(), True)\n",
    "    ])\n",
    "\n",
    "print(REVIEWS_SCHEMA_DEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reviewText` field is not added to the schema here. What do you think will happen if we try to enforce this schema to the `reviews` dataset?\n",
    "1. Spark will enforce manual schema on the defined columns and ignore others\n",
    "2. Spark will enforce manual schema on the defined columns and infer schema for others\n",
    "3. Spark will fail with a SchemaMismatch exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is DataFrame[reviewerID: string, asin: string, reviewerName: string, helpful: array<int>, reviewTime: string, overall: double, summary: string, unixReviewTime: bigint]\n"
     ]
    }
   ],
   "source": [
    "# Try it out\n",
    "reviews = spark.read.json(POSIX, schema=REVIEWS_SCHEMA_DEF)\n",
    "print(\"The answer is {}\".format(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Add `reviewText` field to the data schema (5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(reviewerID,StringType,true),StructField(asin,StringType,true),StructField(reviewerName,StringType,true),StructField(helpful,ArrayType(IntegerType,true),true),StructField(reviewText,StringType,true),StructField(reviewTime,StringType,true),StructField(overall,DoubleType,true),StructField(summary,StringType,true),StructField(unixReviewTime,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Define FULL_REVIEWS_SCHEMA_DEF here\n",
    "\n",
    "FULL_REVIEWS_SCHEMA_DEF = StructType([\n",
    "        StructField('reviewerID', StringType(), True),\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('reviewerName', StringType(), True),\n",
    "        StructField('helpful', ArrayType(\n",
    "                IntegerType(), True), \n",
    "            True),\n",
    "        StructField('reviewText', StringType(), True),\n",
    "        StructField('reviewTime', StringType(), True),\n",
    "        StructField('overall', DoubleType(), True),\n",
    "        StructField('summary', StringType(), True),\n",
    "        StructField('unixReviewTime', LongType(), True)\n",
    "    ])\n",
    "\n",
    "print(FULL_REVIEWS_SCHEMA_DEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(POSIX, schema=FULL_REVIEWS_SCHEMA_DEF)\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame operations\n",
    "\n",
    "Spark DataFrame API allow you to do multiple operations on the Data. The primary advantage of using the DataFrame API is that you can do data transformations with the high level API without having to use Python. Using the high level API has performance advantages.\n",
    "\n",
    "DataFrame API have functionality similar to that of Core RDD API. For example: \n",
    "+ `map`                       : `foreach`, `Select`\n",
    "+ `filter`                    : `filter`\n",
    "+ `groupByKey`, `reduceByKey` : `groupBy`\n",
    "\n",
    "## Selecting Columns\n",
    "\n",
    "You can use SELECT statement to select columns from your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0321732944|    5.0|While many beginn...|              null|A2CX7LUOHB2NDG|    1341100800|\n",
      "|0439886341|    1.0|It never worked. ...|               1.0|A2NWSAGRHCP8N5|    1367193600|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0439886341|    1.0|Do not waste your...|               1.0|A1GI0U4ZRJA8WN|    1334707200|\n",
      "|0511189877|    5.0|Dog got the old r...|              null|A1QGNMC6O1VW39|    1397433600|\n",
      "|0511189877|    2.0|This remote, for ...|               1.0|A3J3BRHTDRFJ2G|    1397433600|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|\n",
      "|0511189877|    5.0|This unit works j...|              null|A34ATBPOK6HCHY|    1395532800|\n",
      "|0511189877|    5.0|It is an exact du...|              null| A89DO69P0XZ27|    1395446400|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|\n",
      "|0528881469|    5.0|Love it has every...|              null|A1DA3W4GTFXP6O|    1405641600|\n",
      "|0528881469|    1.0|I have owned two ...|              null|A29LPQQDG7LD5J|    1352073600|\n",
      "|0528881469|    5.0|We got this GPS f...|              null| AO94DHGC771SJ|    1370131200|\n",
      "|0528881469|    1.0|I'm a professiona...|               0.8| AMO214LNFCEI4|    1290643200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    2.0|Not going to writ...|               0.9|A1H8PY3QHMQQA0|    1290556800|\n",
      "|0528881469|    2.0|My brother is a t...|           0.71875| A2CPBQ5W4OGBX|    1277078400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_df = reviews.select(reviews.asin,\n",
    "                           reviews.overall,\n",
    "                           reviews.reviewText,\n",
    "                           reviews.helpful[0]/reviews.helpful[1],\n",
    "                           reviews.reviewerID,\n",
    "                           reviews.unixReviewTime).\\\n",
    "                    withColumnRenamed('(helpful[0] / helpful[1])','helpful')\n",
    "\n",
    "select_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering rows\n",
    "\n",
    "Filtering lets you select rows based on arguments. The implementation pattern is similar to filtering RDDs, But simpler. Let's filter the reviews that have got an overall score of 3 or more... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0321732944|    5.0|While many beginn...|              null|A2CX7LUOHB2NDG|    1341100800|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0511189877|    5.0|Dog got the old r...|              null|A1QGNMC6O1VW39|    1397433600|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|\n",
      "|0511189877|    5.0|This unit works j...|              null|A34ATBPOK6HCHY|    1395532800|\n",
      "|0511189877|    5.0|It is an exact du...|              null| A89DO69P0XZ27|    1395446400|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|\n",
      "|0528881469|    5.0|Love it has every...|              null|A1DA3W4GTFXP6O|    1405641600|\n",
      "|0528881469|    5.0|We got this GPS f...|              null| AO94DHGC771SJ|    1370131200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "|0528881469|    5.0|I did a lot of co...|               1.0|A37K02NKUIT68K|    1293235200|\n",
      "|0528881469|    4.0|I purchased this ...|               0.5|A2AW1SSVUIYV9Y|    1289001600|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|0.7142857142857143|A2AEHUKOV014BP|    1284249600|\n",
      "|0528881469|    4.0|Well as one of th...|               1.0|A2O8FIJR9EBU56|    1278547200|\n",
      "|0528881469|    4.0|Was fast and what...|              null| AYTBGUX49LF3W|    1398470400|\n",
      "|0528881469|    5.0|We had the GPS fo...|               0.0|A1E4WG8HRWWK4R|    1390867200|\n",
      "|0528881469|    5.0|Back in the old d...|               0.5|A2AOEW5UGXFOOQ|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df = select_df.filter(select_df.overall >= 3)\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the `where` function to filter out rows from `select_df` to get a list of rows *where* `helpful` score is more than or equal to 0.5 (5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0439886341|    1.0|It never worked. ...|               1.0|A2NWSAGRHCP8N5|    1367193600|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0439886341|    1.0|Do not waste your...|               1.0|A1GI0U4ZRJA8WN|    1334707200|\n",
      "|0511189877|    2.0|This remote, for ...|               1.0|A3J3BRHTDRFJ2G|    1397433600|\n",
      "|0528881469|    1.0|I'm a professiona...|               0.8| AMO214LNFCEI4|    1290643200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    2.0|Not going to writ...|               0.9|A1H8PY3QHMQQA0|    1290556800|\n",
      "|0528881469|    2.0|My brother is a t...|           0.71875| A2CPBQ5W4OGBX|    1277078400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "|0528881469|    5.0|I did a lot of co...|               1.0|A37K02NKUIT68K|    1293235200|\n",
      "|0528881469|    4.0|I purchased this ...|               0.5|A2AW1SSVUIYV9Y|    1289001600|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|0.7142857142857143|A2AEHUKOV014BP|    1284249600|\n",
      "|0528881469|    1.0|I was real psyche...|               1.0| AMLFNXUIEMN4T|    1307836800|\n",
      "|0528881469|    4.0|Well as one of th...|               1.0|A2O8FIJR9EBU56|    1278547200|\n",
      "|0528881469|    1.0|Thought the unit ...|               1.0|A3IQGFB959IR4P|    1327363200|\n",
      "|0528881469|    2.0|Twice this item h...|               1.0|A24QFSUU00IZ05|    1293580800|\n",
      "|0528881469|    1.0|DONT WAIST YOUR M...|            0.9375|A1NG5X8VYZWX0Q|    1286582400|\n",
      "|0528881469|    5.0|Back in the old d...|               0.5|A2AOEW5UGXFOOQ|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df = select_df.where(select_df.helpful >= 0.5)\n",
    "filter_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by overall scores\n",
    "\n",
    "Grouping is equivalent to the groupByKey in the core RDD API. You can transform the grouped values using a summary action such as:\n",
    "+ count\n",
    "+ sum\n",
    "+ average\n",
    "+ max and so on ...\n",
    "\n",
    "\n",
    "Lets group the number of reviews by the overall score in the reviews dataset to see the number of entries each score has received. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+\n",
      "|overall|            helpful|count|\n",
      "+-------+-------------------+-----+\n",
      "|    1.0|              0.125|    7|\n",
      "|    1.0|               0.75|   53|\n",
      "|    1.0|0.25925925925925924|    1|\n",
      "|    1.0| 0.5384615384615384|    3|\n",
      "|    1.0| 0.9047619047619048|    2|\n",
      "|    1.0|0.34782608695652173|    1|\n",
      "|    1.0| 0.2631578947368421|    1|\n",
      "|    1.0|0.16666666666666666|   11|\n",
      "|    1.0|               0.95|    3|\n",
      "|    1.0| 0.2857142857142857|    5|\n",
      "|    1.0| 0.4090909090909091|    2|\n",
      "|    1.0|0.14634146341463414|    1|\n",
      "|    1.0| 0.9333333333333333|    5|\n",
      "|    1.0| 0.9545454545454546|    1|\n",
      "|    1.0| 0.8253968253968254|    1|\n",
      "|    1.0| 0.9285714285714286|    1|\n",
      "|    1.0| 0.7083333333333334|    1|\n",
      "|    1.0| 0.5217391304347826|    1|\n",
      "|    1.0| 0.8135593220338984|    1|\n",
      "|    1.0|0.23076923076923078|    1|\n",
      "+-------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped = select_df.groupBy([select_df.overall, select_df.helpful]).count().orderBy(['overall'])\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining DataFrames together\n",
    "\n",
    "You can join two DataFrames together by using a common key. Two different datasets can be used to perform a join. \n",
    "\n",
    "Let's look at the second dataset that stores product details before we dive into joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          categories|\n",
      "+--------------------+\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Electronics, Co...|\n",
      "|[[Sports & Outdoo...|\n",
      "|[[Electronics, Co...|\n",
      "|[[Electronics, Co...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Clothing, Shoes...|\n",
      "|[[Clothing, Shoes...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset2 : Amazon Product information\n",
    "product_filepaths = filepath+'products_*.json'\n",
    "\n",
    "# First, define Schema for second Dataset\n",
    "PRODUCTS_SCHEMA_DEF = StructType([\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('title', StringType(), True),\n",
    "        StructField('price', DoubleType(), True),\n",
    "        StructField('categories', ArrayType(ArrayType(\n",
    "            StringType(), True),True),True)\n",
    "    ])\n",
    "\n",
    "# Load the dataset\n",
    "product_df = spark.read.json(product_filepaths, PRODUCTS_SCHEMA_DEF)\n",
    "\n",
    "product_df.select('categories').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us join the two datasets together based on `asin`. Let's connect all the reviews available to the product details found in the products dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+----------+--------------------+------+--------------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|      asin|               title| price|          categories|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+----------+--------------------+------+--------------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|0132793040|Kelby Training DV...|  null|[[Electronics, Co...|\n",
      "|0439886341|    1.0|It never worked. ...|               1.0|A2NWSAGRHCP8N5|    1367193600|0439886341|Digital Organizer...|  8.15|[[Electronics, Co...|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|0439886341|Digital Organizer...|  8.15|[[Electronics, Co...|\n",
      "|0439886341|    1.0|Do not waste your...|               1.0|A1GI0U4ZRJA8WN|    1334707200|0439886341|Digital Organizer...|  8.15|[[Electronics, Co...|\n",
      "|0511189877|    2.0|This remote, for ...|               1.0|A3J3BRHTDRFJ2G|    1397433600|0511189877|CLIKR-5 Time Warn...| 23.36|[[Electronics, Ac...|\n",
      "|0528881469|    1.0|I'm a professiona...|               0.8| AMO214LNFCEI4|    1290643200|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    2.0|Not going to writ...|               0.9|A1H8PY3QHMQQA0|    1290556800|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    2.0|My brother is a t...|           0.71875| A2CPBQ5W4OGBX|    1277078400|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    5.0|I did a lot of co...|               1.0|A37K02NKUIT68K|    1293235200|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    4.0|I purchased this ...|               0.5|A2AW1SSVUIYV9Y|    1289001600|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|0.7142857142857143|A2AEHUKOV014BP|    1284249600|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    1.0|I was real psyche...|               1.0| AMLFNXUIEMN4T|    1307836800|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    4.0|Well as one of th...|               1.0|A2O8FIJR9EBU56|    1278547200|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    1.0|Thought the unit ...|               1.0|A3IQGFB959IR4P|    1327363200|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    2.0|Twice this item h...|               1.0|A24QFSUU00IZ05|    1293580800|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    1.0|DONT WAIST YOUR M...|            0.9375|A1NG5X8VYZWX0Q|    1286582400|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "|0528881469|    5.0|Back in the old d...|               0.5|A2AOEW5UGXFOOQ|    1294790400|0528881469|Rand McNally 5288...|299.99|[[Electronics, GP...|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+----------+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_reviews = (filter_df.join(product_df, \n",
    "                                  product_df.asin == filter_df.asin).\n",
    "                              dropna(subset=\"title\"))\n",
    "product_reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you join two RDDs, you have to reshape the data into (k,v) pairs where the key is the join key. This may involve two additional map transformations. This is not necessary when we join data using DataFrames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Similar to Pandas, DataFrames come equipped with functions to address missing data.\n",
    "+ `dropna` function: can be used to remove observations with missing values\n",
    "+ `fillna` function: can be used to fill missing values with a default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dropna` function can be used to remove observations where helpful factor is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_products_df = product_df.dropna(subset=[\"price\"])\n",
    "dense_products_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`na.drop()` function is an alias to same behaviour within DataFrames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_products_df_2 = product_df.na.drop(subset=[\"price\"])\n",
    "dense_products_df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fillna` function can be used to replace a set of missing values with a sensible alternative values (eg: a default) by first figuring out an ideal value as a default value for the helpful field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(helpful)|\n",
      "+------------------+\n",
      "|0.7383332842163571|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "average = select_df.select(mean(select_df.helpful))\n",
    "average.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the `select` function with other summarisation functions to summarise values in the table in 3 columns as follows (5 mins):\n",
    "\n",
    "    1) average helpful score for reviews\n",
    "    \n",
    "    2) number of reviews in the dataframe\n",
    "    \n",
    "    3) maximum overall score for a review amongst all reviews\n",
    "    \n",
    "**Hint: `pyspark.sql.functions` module contains various useful functions**. Full set of functions can be found at http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, count, max\n",
    "summary = select_df.select(mean(select_df.helpful), \n",
    "                           count(select_df.asin), \n",
    "                           max(select_df.overall))\n",
    "summary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0321732944|    5.0|While many beginn...|0.7383332842163571|A2CX7LUOHB2NDG|    1341100800|\n",
      "|0439886341|    1.0|It never worked. ...|               1.0|A2NWSAGRHCP8N5|    1367193600|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0439886341|    1.0|Do not waste your...|               1.0|A1GI0U4ZRJA8WN|    1334707200|\n",
      "|0511189877|    5.0|Dog got the old r...|0.7383332842163571|A1QGNMC6O1VW39|    1397433600|\n",
      "|0511189877|    2.0|This remote, for ...|               1.0|A3J3BRHTDRFJ2G|    1397433600|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|\n",
      "|0511189877|    5.0|This unit works j...|0.7383332842163571|A34ATBPOK6HCHY|    1395532800|\n",
      "|0511189877|    5.0|It is an exact du...|0.7383332842163571| A89DO69P0XZ27|    1395446400|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|\n",
      "|0528881469|    5.0|Love it has every...|0.7383332842163571|A1DA3W4GTFXP6O|    1405641600|\n",
      "|0528881469|    1.0|I have owned two ...|0.7383332842163571|A29LPQQDG7LD5J|    1352073600|\n",
      "|0528881469|    5.0|We got this GPS f...|0.7383332842163571| AO94DHGC771SJ|    1370131200|\n",
      "|0528881469|    1.0|I'm a professiona...|               0.8| AMO214LNFCEI4|    1290643200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    2.0|Not going to writ...|               0.9|A1H8PY3QHMQQA0|    1290556800|\n",
      "|0528881469|    2.0|My brother is a t...|           0.71875| A2CPBQ5W4OGBX|    1277078400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_value = 0.7383332842163571\n",
    "dense_reviews_df = select_df.fillna(default_value, subset=[\"helpful\"])\n",
    "dense_reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0321732944|    5.0|While many beginn...|               0.0|A2CX7LUOHB2NDG|    1341100800|\n",
      "|0439886341|    1.0|It never worked. ...|               1.0|A2NWSAGRHCP8N5|    1367193600|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0439886341|    1.0|Do not waste your...|               1.0|A1GI0U4ZRJA8WN|    1334707200|\n",
      "|0511189877|    5.0|Dog got the old r...|               0.0|A1QGNMC6O1VW39|    1397433600|\n",
      "|0511189877|    2.0|This remote, for ...|               1.0|A3J3BRHTDRFJ2G|    1397433600|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|\n",
      "|0511189877|    5.0|This unit works j...|               0.0|A34ATBPOK6HCHY|    1395532800|\n",
      "|0511189877|    5.0|It is an exact du...|               0.0| A89DO69P0XZ27|    1395446400|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|\n",
      "|0528881469|    5.0|Love it has every...|               0.0|A1DA3W4GTFXP6O|    1405641600|\n",
      "|0528881469|    1.0|I have owned two ...|               0.0|A29LPQQDG7LD5J|    1352073600|\n",
      "|0528881469|    5.0|We got this GPS f...|               0.0| AO94DHGC771SJ|    1370131200|\n",
      "|0528881469|    1.0|I'm a professiona...|               0.8| AMO214LNFCEI4|    1290643200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    2.0|Not going to writ...|               0.9|A1H8PY3QHMQQA0|    1290556800|\n",
      "|0528881469|    2.0|My brother is a t...|           0.71875| A2CPBQ5W4OGBX|    1277078400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_value = .0\n",
    "dense_reviews_df_2 = select_df.na.fill(default_value, subset=[\"helpful\"])\n",
    "dense_reviews_df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Spark SQL\n",
    "\n",
    "Spark DataFrames also allow you to use Spark SQL to query from Petabytes of data. Spark comes with a SQL like query language which can be used to query from Distributed DataFrames. A key advantage of using Spark SQL is the use of [Catalyst query optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Catalyst\") and [Project Tungsten](https://databricks.com/session/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal) under the hood to convert your SQL query to run it most efficiently. Spark converts the SQL syntax to a DAG that can be executed to realize the desired result.\n",
    "\n",
    "### Example Queries\n",
    "\n",
    "Spark SQL can leverage the same functionality as the DataFrame API provides. In fact, it provides more functionality via SQL capabilities and HQL capabilities that are available to Spark SQL environment. \n",
    "\n",
    "Lets look at different functions available in Spark SQL environment by using examples that use multiple functions. This will benefit by:\n",
    "+ Covering many functions that are possible via spark SQL\n",
    "+ Giving an understanding about how to pipe multiple functions together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we register the dataframes we have in SQLContext to introduce references that we can use to do transformations to the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30000 reviews about 2469 products\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrames to be used in sql\n",
    "dense_reviews_df.createOrReplaceTempView(\"reviews\")\n",
    "product_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"There are {0} reviews about {1} products\".format(dense_reviews_df.count(),product_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------+\n",
      "|      asin|overall|          reviewText| price|\n",
      "+----------+-------+--------------------+------+\n",
      "|0528881469|    5.0|Love it has every...|299.99|\n",
      "|0528881469|    1.0|I have owned two ...|299.99|\n",
      "|0528881469|    5.0|We got this GPS f...|299.99|\n",
      "|0528881469|    1.0|I'm a professiona...|299.99|\n",
      "|0528881469|    4.0|This is a great t...|299.99|\n",
      "|0528881469|    3.0|Well, what can I ...|299.99|\n",
      "|0528881469|    2.0|Not going to writ...|299.99|\n",
      "|0528881469|    2.0|My brother is a t...|299.99|\n",
      "|0528881469|    4.0|This unit is a fa...|299.99|\n",
      "|0528881469|    5.0|I did a lot of co...|299.99|\n",
      "|0528881469|    4.0|I purchased this ...|299.99|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|299.99|\n",
      "|0528881469|    1.0|I was real psyche...|299.99|\n",
      "|0528881469|    4.0|Well as one of th...|299.99|\n",
      "|0528881469|    1.0|Thought the unit ...|299.99|\n",
      "|0528881469|    4.0|Was fast and what...|299.99|\n",
      "|0528881469|    2.0|Twice this item h...|299.99|\n",
      "|0528881469|    1.0|DONT WAIST YOUR M...|299.99|\n",
      "|0528881469|    5.0|We had the GPS fo...|299.99|\n",
      "|0528881469|    5.0|Back in the old d...|299.99|\n",
      "+----------+-------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT reviews.asin, overall, reviewText, price\n",
    "            FROM reviews JOIN products ON reviews.asin=products.asin\n",
    "            WHERE price > 50.00\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "As a data scientist, imagine you hypothesize the following claim: \n",
    "\n",
    "*The price of goods and the overall score of reviews are correlated.*\n",
    "\n",
    "What is a good experiment to quickly sanity check if this is True??\n",
    "\n",
    "To support this experiment, formulate a SQL query that calculates the average product price score for each distinct overall score value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|overall|        avg(price)|\n",
      "+-------+------------------+\n",
      "|    1.0|47.950229775011934|\n",
      "|    2.0|47.709516129032274|\n",
      "|    3.0| 44.33610311750602|\n",
      "|    4.0|44.286331981874596|\n",
      "|    5.0| 41.57316541722106|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT overall, avg(price)\n",
    "            FROM reviews JOIN products ON reviews.asin=products.asin\n",
    "            GROUP BY overall\n",
    "            ORDER BY overall \n",
    "\"\"\"\n",
    "\n",
    "result_2 = spark.sql(sql_query)\n",
    "result_2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions\n",
    "\n",
    "Spark SQL also provides the functionality similar to User Defined Functions (UDF) offering in Hive. Spark uses `udf.register()` function to register python functions in a Spark Session.\n",
    "\n",
    "You can register user defined functions with/ without a return type.\n",
    "\n",
    "In the following example, a python function that uses regex and `str.lower()` function is introduced to SQLContext under the reference `sanitise()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      asin|          reviewText|             cleaned|\n",
      "+----------+--------------------+--------------------+\n",
      "|0528881469|Love it has every...|love it has every...|\n",
      "|0528881469|I have owned two ...|i have owned two ...|\n",
      "|0528881469|We got this GPS f...|we got this gps f...|\n",
      "|0528881469|I'm a professiona...|im a professional...|\n",
      "|0528881469|This is a great t...|this is a great t...|\n",
      "|0528881469|Well, what can I ...|well what can i s...|\n",
      "|0528881469|Not going to writ...|not going to writ...|\n",
      "|0528881469|My brother is a t...|my brother is a t...|\n",
      "|0528881469|This unit is a fa...|this unit is a fa...|\n",
      "|0528881469|I did a lot of co...|i did a lot of co...|\n",
      "|0528881469|I purchased this ...|i purchased this ...|\n",
      "|0528881469|EXCELLENT. BEST T...|excellent best tr...|\n",
      "|0528881469|I was real psyche...|i was real psyche...|\n",
      "|0528881469|Well as one of th...|well as one of th...|\n",
      "|0528881469|Thought the unit ...|thought the unit ...|\n",
      "|0528881469|Was fast and what...|was fast and what...|\n",
      "|0528881469|Twice this item h...|twice this item h...|\n",
      "|0528881469|DONT WAIST YOUR M...|dont waist your m...|\n",
      "|0528881469|We had the GPS fo...|we had the gps fo...|\n",
      "|0528881469|Back in the old d...|back in the old d...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def transform_review(review):\n",
    "    \"\"\" takes a review string and cleans it by\n",
    "        1) removing non-alphanumeric charactors\n",
    "        2) turning all alphabetic charactors to lower case\n",
    "        \n",
    "        Args:\n",
    "            review (str): review text\n",
    "        \n",
    "        Returns:\n",
    "            str: sanitised review text\n",
    "    \"\"\"\n",
    "    x1 = re.sub('[^0-9a-zA-Z\\s]+', '', review)\n",
    "    return x1.lower()\n",
    "\n",
    "result.createOrReplaceTempView(\"result\")\n",
    "\n",
    "# register the UDF\n",
    "spark.udf.register(\"sanitise\", transform_review, returnType=StringType())\n",
    "\n",
    "# use the UDF in SQL \n",
    "sql_query_transform = \"\"\"SELECT asin, reviewText, sanitise(reviewText) as cleaned\n",
    "            FROM result\n",
    "\"\"\"\n",
    "\n",
    "result_transform = spark.sql(sql_query_transform)\n",
    "result_transform.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise\n",
    "\n",
    "You can see from the data that Categories variable is a list of lists of categories. In the list of categories, each list represents a product category where the collection of strings represent the hierarchical structure of the categories. \n",
    "\n",
    "eg: `[\"Women's Fasion\", \"Eveningwear\", \"Dress\"]` \n",
    "\n",
    "        represents the category  `Women's Fasion --> Eveningwear --> Dress`\n",
    "\n",
    "In terms of dealing with these categories, it is easier for us to use this data if the category field was a flat list of string where each string is a unique product category\n",
    "\n",
    "Implement a python function that takes in a list of lists of strings and convert it into a flat list of strings. Every string in this flat list is a **unique** product category where the hierarchy within the category is shown with ` > ` symbol.\n",
    "\n",
    "eg: `[\"Women's Fasion\", \"Eveningwear\", \"Dress\"]` --> `\"Women's Fasion > Eveningwear > Dress\"`\n",
    "\n",
    "Now use this function to transform the `categories` field in `product_df` (15 Mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPERATOR = \" > \"\n",
    "\n",
    "def flatten_unique_categories(categories):\n",
    "    \"\"\"Takes in a list of categories and returns back the flattened unique set of categories\n",
    "    assigned to specific product. It does the following:\n",
    "        1) connects the strings in the hierarchical category using the seperator\n",
    "        2) generates a list of distinct categories per record\n",
    "    \n",
    "    Args:\n",
    "        categories ([[str]]): a list of categories where every category is a list of strings\n",
    "        \n",
    "    Returns:\n",
    "        flattened_unique_cats ([str]): list of unique categories where the heierarchy within the\n",
    "                                      category is preserved using a ' > ' charactor                                      \n",
    "    \"\"\"\n",
    "    # 1. join the strings in category heierarchy using the SEPERATOR\n",
    "    flat_cats = [SEPERATOR.join(cat) for cat in categories]\n",
    "\n",
    "    # 2. get the unique categories and return them in a list\n",
    "    flattened_unique_cats = list(set(flat_cats))\n",
    "\n",
    "    # return the result\n",
    "    return flattened_unique_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b > c', 'a > b > c', 'a > c']\n"
     ]
    }
   ],
   "source": [
    "test = [list(\"abc\"), list(\"bc\"), list(\"ac\"), list(\"bc\")]\n",
    "\n",
    "actual = flatten_unique_categories(test)\n",
    "\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use this function to transform `categories` field in `product_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|asin      |title                                                                                                                            |price|flattened                                                                                                                                                                                                                                                                              |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0000037214|Purple Sequin Tiny Dancer Tutu Ballet Dance Fairy Princess Costume Accessory                                                     |6.99 |[Clothing, Shoes & Jewelry > Girls, Clothing, Shoes & Jewelry > Novelty, Costumes & More > Costumes & Accessories > More Accessories > Kids & Baby]                                                                                                                                    |\n",
      "|0000032069|Adult Ballet Tutu Cheetah Pink                                                                                                   |7.89 |[Sports & Outdoors > Other Sports > Dance > Clothing > Girls > Skirts]                                                                                                                                                                                                                 |\n",
      "|0000031909|Girls Ballet Tutu Neon Pink                                                                                                      |7.0  |[Sports & Outdoors > Other Sports > Dance]                                                                                                                                                                                                                                             |\n",
      "|0000032034|Adult Ballet Tutu Yellow                                                                                                         |7.87 |[Sports & Outdoors > Other Sports > Dance > Clothing > Girls > Skirts]                                                                                                                                                                                                                 |\n",
      "|0000031852|Girls Ballet Tutu Zebra Hot Pink                                                                                                 |3.17 |[Sports & Outdoors > Other Sports > Dance]                                                                                                                                                                                                                                             |\n",
      "|0000032050|Adult Ballet Tutu Purple                                                                                                         |12.85|[Sports & Outdoors > Other Sports > Dance > Clothing > Girls > Skirts]                                                                                                                                                                                                                 |\n",
      "|0000031887|Ballet Dress-Up Fairy Tutu                                                                                                       |6.79 |[Clothing, Shoes & Jewelry > Girls > Clothing > Active > Active Skirts]                                                                                                                                                                                                                |\n",
      "|0000031895|Girls Ballet Tutu Neon Blue                                                                                                      |2.99 |[Sports & Outdoors > Other Sports > Dance]                                                                                                                                                                                                                                             |\n",
      "|0123456479|SHINING IMAGE HUGE PINK LEATHER JEWELRY BOX / CASE / STORAGE / ORGANIZER WITH TRAVEL CASE AND LOCK                               |64.98|[Clothing, Shoes & Jewelry > Novelty, Costumes & More > Jewelry Accessories > Jewelry Boxes & Organizers > Jewelry Boxes]                                                                                                                                                              |\n",
      "|0132793040|Kelby Training DVD: Mastering Blend Modes in Adobe Photoshop CS5 By Corey Barker                                                 |null |[Electronics > Computers & Accessories > Cables & Accessories > Monitor Accessories]                                                                                                                                                                                                   |\n",
      "|0188477284|Klean Kanteen Classic Stainless Steel Water Bottle 18 oz with Loop Cap (Reef Blue)                                               |null |[Sports & Outdoors > Accessories > Sports Water Bottles]                                                                                                                                                                                                                               |\n",
      "|0321732944|Kelby Training DVD: Adobe Photoshop CS5 Crash Course By Matt Kloskowski                                                          |null |[Electronics > Computers & Accessories > Cables & Accessories > Monitor Accessories]                                                                                                                                                                                                   |\n",
      "|0439886341|Digital Organizer and Messenger                                                                                                  |8.15 |[Electronics > Computers & Accessories > PDAs, Handhelds & Accessories > PDAs & Handhelds]                                                                                                                                                                                             |\n",
      "|0456844570|RiZ Women's Beautify Crafted &frac12; Rimmed Floral Clubmaster Sunglasses                                                        |null |[Clothing, Shoes & Jewelry > Novelty, Costumes & More > Band & Music Fan > Accessories, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]                                                                                               |\n",
      "|0456808574|Lantin White Visor Wrap Around Ski Style Aviator Sunglasses with Black Lenses                                                    |null |[Clothing, Shoes & Jewelry > Men > Accessories > Sunglasses & Eyewear Accessories > Sunglasses, Clothing, Shoes & Jewelry > Novelty, Costumes & More > Band & Music Fan > Accessories, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]|\n",
      "|0456830197|NVC Unisex Light Weight Silver 'Dakota' Glasses Case with Brushed Metal Finish                                                   |null |[Clothing, Shoes & Jewelry > Men > Accessories > Sunglasses & Eyewear Accessories > Sunglasses, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]                                                                                       |\n",
      "|0456856293|Kismeth Eyewear Classic Large Top Gun Aviator Sunglasses with Gold Frames &amp; Green Smoked Lenses                              |null |[Clothing, Shoes & Jewelry > Men > Accessories > Sunglasses & Eyewear Accessories > Sunglasses, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]                                                                                       |\n",
      "|0456840532|Max-MPH Black - Large Wayfarer Sunglasses Available in Black with Extra Dark Lenses &amp; Black with Clear (No Strength) Lenses  |null |[Clothing, Shoes & Jewelry > Men > Accessories > Sunglasses & Eyewear Accessories > Sunglasses, Clothing, Shoes & Jewelry > Novelty, Costumes & More > Band & Music Fan > Accessories, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]|\n",
      "|0456787283|FX1 Small Adult Aviator Sunglasses with Silver Frames &amp; Fully Mirrored Lenses Offering Full UV400 Protection Cat 4 Lenses    |null |[Clothing, Shoes & Jewelry > Men > Accessories > Sunglasses & Eyewear Accessories > Sunglasses, Clothing, Shoes & Jewelry > Novelty, Costumes & More > Band & Music Fan > Accessories, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]|\n",
      "|0456838384|Riz Small Unisex (Mens/ Womens) Black Classic Wayfarer Sunglasses with Gradient Smoked Lenses - NEW 'Cool Blue Technology' Lenses|null |[Clothing, Shoes & Jewelry > Men > Accessories > Sunglasses & Eyewear Accessories > Sunglasses, Clothing, Shoes & Jewelry > Novelty, Costumes & More > Band & Music Fan > Accessories, Clothing, Shoes & Jewelry > Women > Accessories > Sunglasses & Eyewear Accessories > Sunglasses]|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the dataframe as a table\n",
    "\n",
    "product_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# 1. register the UDF\n",
    "\n",
    "spark.udf.register(\"flatten_unique\", \n",
    "                   flatten_unique_categories, \n",
    "                   returnType=ArrayType(StringType()))\n",
    "\n",
    "# 2. use the UDF in SQL \n",
    "\n",
    "sql_query_flatten = \"\"\"SELECT asin, title, price, flatten_unique(categories) as flattened\n",
    "                        FROM products\n",
    "                    \"\"\"\n",
    "\n",
    "result_flatten = spark.sql(sql_query_flatten)\n",
    "result_flatten.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 5, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,4,5,6,1]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "al = list(set(a))\n",
    "print(al)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
